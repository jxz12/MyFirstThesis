{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy.sparse import find\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, cut_tree, to_tree, ClusterNode\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import wasserstein_distance, entropy\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "import numpy.linalg as lalg\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from pickle import dump, load, HIGHEST_PROTOCOL\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wrangle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_connected_component(G):\n",
    "    G = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "    G = nx.relabel.convert_node_labels_to_integers(G, label_attribute='orig')\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_karate():\n",
    "    mat = loadmat('karate.mat')\n",
    "    S = mat['Problem'][0][0][2]\n",
    "    G = nx.Graph(S)\n",
    "\n",
    "    mr_hi = set([1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 17, 18, 20, 22])\n",
    "    split = {}\n",
    "    cols = {}\n",
    "    for i in G.nodes:\n",
    "        split[i] = 0 if i+1 in mr_hi else 1\n",
    "        cols[i] = (1,0,0,1) if i+1 in mr_hi else (0,0,0,1)\n",
    "    nx.set_node_attributes(G, split, 'cluster')\n",
    "    nx.set_node_attributes(G, cols, 'color')\n",
    "    G.graph['name'] = 'karate'\n",
    "    return largest_connected_component(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_football():\n",
    "    # data fixed by Tim Evans\n",
    "    mat = loadmat('football_fixed.mat')\n",
    "    I,J = mat['links']\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(zip(I,J))\n",
    "\n",
    "    confs = {}\n",
    "    cols = {}\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    for node in mat['nodes']:\n",
    "        idx = int(node[0])\n",
    "        conf = int(node[2])\n",
    "        assert idx not in confs.keys()\n",
    "        if conf>10:\n",
    "            confs[idx] = conf\n",
    "            cols[idx] = (0,0,0,1)\n",
    "        else:\n",
    "            confs[idx] = conf\n",
    "            cols[idx] = cmap(conf/11)\n",
    "    nx.set_node_attributes(G, confs, 'cluster')\n",
    "    nx.set_node_attributes(G, cols, 'color')\n",
    "    G.graph['name'] = 'football'\n",
    "    return largest_connected_component(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_caltech():\n",
    "    mat = loadmat('Caltech36.mat')\n",
    "    G = nx.Graph(mat['A'])\n",
    "\n",
    "    houses = {}\n",
    "    cols = {}\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    for i,x in enumerate(mat['local_info']):\n",
    "        house = x[4]\n",
    "        if house==0:\n",
    "            houses[i] = house\n",
    "            cols[i] = (0,0,0,1)\n",
    "        else:\n",
    "            houses[i] = house\n",
    "            cols[i] = cmap((house-165)/10)\n",
    "    nx.set_node_attributes(G, houses, 'cluster')\n",
    "    nx.set_node_attributes(G, cols, 'color')\n",
    "    G.graph['name'] = 'caltech'\n",
    "    return largest_connected_component(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_lesmis():\n",
    "    G = largest_connected_component(nx.les_miserables_graph())\n",
    "    # no ground truth\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    nx.set_node_attributes(G, {i:0 for i in G.nodes}, 'cluster')\n",
    "    nx.set_node_attributes(G, {i:cmap(0) for i in G.nodes}, 'color')\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lfr():\n",
    "    G = LFR_benchmark_graph(n=250, tau1=3, tau2=1.5, mu=.1, average_degree=5, min_community=20, seed=0)\n",
    "    communities = nx.get_node_attributes(G, 'community')\n",
    "    seen = set()\n",
    "    clust_idx = 0\n",
    "    clusts = {}\n",
    "    cols = {}\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    for idx,community in communities.items():\n",
    "        if idx not in seen:\n",
    "            for member in community:\n",
    "                clusts[member] = clust_idx\n",
    "                cols[member] = cmap(clust_idx/10)\n",
    "                seen.add(member)\n",
    "            clust_idx += 1\n",
    "    nx.set_node_attributes(G, clusts, 'cluster')\n",
    "    nx.set_node_attributes(G, cols, 'color')\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    G.graph['name'] = 'lfr'\n",
    "    return largest_connected_component(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_metric(dist, eps=0):\n",
    "    square = squareform(dist)\n",
    "    n,m = square.shape\n",
    "    assert n==m\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                if square[i,j] > square[i,k]+square[k,j]+eps:\n",
    "                    print(f'{i} {j} {k}')\n",
    "                    print(square[i,j])\n",
    "                    print(square[i,k]+square[k,j])\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def draw_dendrogram(G, hierarchy, figsize=(20,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "    dend = dendrogram(hierarchy, leaf_rotation=90, leaf_font_size=8)\n",
    "\n",
    "    ticks, labels = plt.xticks()\n",
    "    for label in labels:\n",
    "        idx = int(label.get_text())\n",
    "        label.set_color(G.nodes[idx]['color'])\n",
    "    plt.xticks(ticks, labels)\n",
    "\n",
    "def eval_ARI(G, hierarchy):\n",
    "    \"\"\"calculates the adjusted rand index to evaluate cluster quality\"\"\"\n",
    "    if hierarchy is None:\n",
    "        return (None, 'infinity') # might happen if there are infinite distances so KL is undefined\n",
    "    try:\n",
    "        cut = cut_tree(hierarchy).transpose()\n",
    "    except ValueError:\n",
    "        return (None, 'non-metric') # might happen if there are negative distances in tree due to non-metric\n",
    "    \n",
    "    attrs = nx.get_node_attributes(G, 'cluster')\n",
    "    n = len(G)\n",
    "    gtruth = []\n",
    "    ignore = set() # unuseful labels are set to -1 earlier\n",
    "    for idx,label in attrs.items():\n",
    "        if label >= 0:\n",
    "            gtruth.append(label)\n",
    "        else:\n",
    "            ignore.add(idx)\n",
    "    labelset = set(gtruth)\n",
    "    predicted = []\n",
    "    \n",
    "    # try cutting at every dendrogram branch\n",
    "    best = -float('inf')\n",
    "    argbest = -1\n",
    "    for n_clust, clusters in enumerate(cut):\n",
    "        score = adjusted_rand_score(gtruth, clusters)\n",
    "        if score > best:\n",
    "            best = score\n",
    "            argbest = n_clust\n",
    "    return best, n-argbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to create high dimensional embeddings based on markov chains\n",
    "\n",
    "def embed_snapshot_markov(G, steps, degrees):\n",
    "    '''embedding of pons & lapaty, including extra weighting using degrees'''\n",
    "    # get markov matrix P\n",
    "    n = len(G)\n",
    "    A = nx.to_numpy_array(G)\n",
    "    D = np.identity(n) * np.sum(A, axis=1)\n",
    "    P = lalg.inv(D) @ A\n",
    "    \n",
    "    # steps\n",
    "    P = lalg.matrix_power(P, steps)\n",
    "    if degrees:\n",
    "        P = lalg.inv(sqrtm(D)) @ P\n",
    "    return P\n",
    "\n",
    "def embed_damped_markov(G, damping, steps=100):\n",
    "    '''weights each markov chain state according to a damping factor'''\n",
    "    # get markov matrix P\n",
    "    n = len(G)\n",
    "    A = nx.to_numpy_array(G)\n",
    "    D = np.identity(n) * np.sum(A, axis=1)\n",
    "    P = lalg.inv(D) @ A\n",
    "    \n",
    "    # get weighted sum of all snapshots\n",
    "    state = np.array(P)\n",
    "    final = np.array(P)\n",
    "    totaldamp = 1\n",
    "    currdamp = damping\n",
    "    for i in range(1,steps):\n",
    "        state = state @ P\n",
    "        final += currdamp * state\n",
    "        totaldamp += currdamp\n",
    "        currdamp *= damping\n",
    "    final /= totaldamp\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linkage methods\n",
    "\n",
    "def cluster_walktrap(embedding, method='ward', opt_ord=False):\n",
    "    dist = pdist(embedding, metric='euclidean')\n",
    "    # print(is_metric(dist))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "    \n",
    "def KL_symmetric(embedding, method='ward', opt_ord=False):\n",
    "    if min(np.nditer(embedding)) <= 0:\n",
    "        return None\n",
    "    dist = pdist(embedding, metric=lambda x,y: entropy(x,y)+entropy(y,x))\n",
    "    # print(is_metric(dist))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "\n",
    "def wasserstein(embedding, method='ward', opt_ord=False):\n",
    "    dist = pdist(embedding, wasserstein_distance)\n",
    "    # print(is_metric(squareform(dist), 1e-10))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "\n",
    "def bacon(G, method='ward', opt_ord=False):\n",
    "    A = nx.to_numpy_array(G)\n",
    "    bacon = dijkstra(A, directed=False, unweighted=True)\n",
    "    dist = squareform(bacon)\n",
    "\n",
    "    # print(is_metric(dist))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "\n",
    "def jaccard(G, method='ward', opt_ord=False):\n",
    "    A = nx.to_numpy_array(G)\n",
    "    dist = pdist(A, metric='jaccard')\n",
    "\n",
    "    # print(is_metric(dist))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "\n",
    "def bacon_jaccard(G, method='ward', opt_ord=False):\n",
    "    A = nx.to_numpy_array(G)\n",
    "    jacc = pdist(A, metric='jaccard')\n",
    "    bacon = dijkstra(A, directed=False, unweighted=True)\n",
    "    dist = jacc * squareform(bacon)\n",
    "\n",
    "    # print(is_metric(dist))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)\n",
    "\n",
    "def commute(G, method='ward', opt_ord=False):\n",
    "    # first get unnormalised laplacian\n",
    "    n = len(G)\n",
    "    A = nx.to_numpy_array(G)\n",
    "    D = np.identity(n) * np.sum(A, axis=1)\n",
    "    L = D-A\n",
    "    \n",
    "    # get moore-penrose pseudo-inverse\n",
    "    Lplus = lalg.pinv(L)\n",
    "    vol = 2*len(G.edges)\n",
    "    \n",
    "    # use to calculate ECT\n",
    "    def commute(i,j):\n",
    "        return vol * (Lplus[i,i] + Lplus[j,j] - 2*Lplus[i,j])\n",
    "    \n",
    "    dist = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dist.append(commute(i,j))\n",
    "            \n",
    "#     print(is_metric(dist, 1e-5))\n",
    "    return linkage(dist, method=method, optimal_ordering=opt_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert \n",
    "\n",
    "def to_linkage(root):\n",
    "    nbranches = 0\n",
    "    nleaves = 0\n",
    "    def count(node):\n",
    "        assert node is not None\n",
    "        if node.left is None and node.right is None:\n",
    "            nonlocal nleaves\n",
    "            nleaves += 1\n",
    "            return\n",
    "        nonlocal nbranches\n",
    "        nbranches += 1\n",
    "        count(node.left)\n",
    "        count(node.right)\n",
    "    \n",
    "    count(root)\n",
    "    linkage = np.zeros((nbranches,4))\n",
    "    \n",
    "    def link(node):\n",
    "        assert node is not None\n",
    "        if node.left is None and node.right is None:\n",
    "            return\n",
    "        linkage[node.id-nleaves,0] = node.left.id\n",
    "        linkage[node.id-nleaves,1] = node.right.id\n",
    "        linkage[node.id-nleaves,2] = node.dist\n",
    "        linkage[node.id-nleaves,3] = node.count\n",
    "        link(node.left)\n",
    "        link(node.right)\n",
    "        \n",
    "    link(root)\n",
    "    return linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divisive clustering of girvan and newman\n",
    "\n",
    "def girvan_newman(G):\n",
    "    G = G.copy() # required because this function destroys the graph\n",
    "    n = nx.number_of_nodes(G)\n",
    "    m = nx.number_of_edges(G)\n",
    "    \n",
    "    clust_idx = 2*n - 2\n",
    "    root = ClusterNode(clust_idx, dist=m, count=n)\n",
    "    clusters = {}\n",
    "    for i in range(n):\n",
    "        clusters[i] = root\n",
    "    \n",
    "    ncc_old = 1\n",
    "    for k in range(m):\n",
    "        betweenness = nx.edge_betweenness_centrality(G)\n",
    "        most_between = max(betweenness, key=lambda x: betweenness[x])\n",
    "        i = most_between[0]\n",
    "        j = most_between[1]\n",
    "        G.remove_edge(i,j)\n",
    "        \n",
    "        ncc = nx.number_connected_components(G)\n",
    "        if ncc > ncc_old:\n",
    "            \n",
    "            clust = clusters[i]\n",
    "            clust_2 = clusters[j]\n",
    "            if clust != clust_2:\n",
    "                raise Exception(\"sumting wong\")\n",
    "                \n",
    "            comp_1 = nx.node_connected_component(G, i)\n",
    "            comp_2 = nx.node_connected_component(G, j)\n",
    "            \n",
    "            if len(comp_1) < len(comp_2):\n",
    "                i,j = j,i\n",
    "                comp_1,comp_2 = comp_2,comp_1\n",
    "            \n",
    "            if len(comp_1) == 1:\n",
    "                clust.left = ClusterNode(i, dist=0, count=1)\n",
    "            else:\n",
    "                clust_idx -= 1\n",
    "                clust.left = ClusterNode(clust_idx, dist=m-k-1, count=len(comp_1))\n",
    "                \n",
    "            if len(comp_2) == 1:\n",
    "                clust.right = ClusterNode(j, dist=0, count=1)\n",
    "            else:\n",
    "                clust_idx -= 1\n",
    "                clust.right = ClusterNode(clust_idx, dist=m-k-1, count=len(comp_2))\n",
    "            \n",
    "            for idx in comp_1:\n",
    "                clusters[idx] = clust.left\n",
    "            for idx in comp_2:\n",
    "                clusters[idx] = clust.right\n",
    "                \n",
    "        ncc_old = ncc\n",
    "            \n",
    "    return to_linkage(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big slow function to measure quality of every clustering method\n",
    "\n",
    "def do_experiment(G, method):\n",
    "    # jaccard * bacon\n",
    "    results = {}\n",
    "    results['bacon'] = eval_ARI(G, bacon(G, method, False))\n",
    "    results['jacc'] = eval_ARI(G, jaccard(G, method, False))\n",
    "    results['baconjacc'] = eval_ARI(G, bacon_jaccard(G, method, False))\n",
    "\n",
    "    # euclidean commute time\n",
    "    results['commute'] = eval_ARI(G, commute(G, method, False))\n",
    "\n",
    "    # walktrap, KL, wasserstein (all with both types of embedding)\n",
    "    all_steps = [1,2,3,4,5,6,7,8,9,10,100]\n",
    "    for steps in all_steps:\n",
    "        embedding = embed_snapshot_markov(G, steps, degrees=True)\n",
    "        results[f'snapshotD_walktrap_{steps:03d}'] = eval_ARI(G, cluster_walktrap(embedding, method, False))\n",
    "        results[f'snapshotD_KL_{steps:03d}'] = eval_ARI(G, KL_symmetric(embedding, method, False))\n",
    "        results[f'snapshotD_wasserstein_{steps:03d}'] = eval_ARI(G, wasserstein(embedding, method, False))\n",
    "\n",
    "        embedding = embed_snapshot_markov(G, steps, degrees=False)\n",
    "        results[f'snapshotI_walktrap_{steps:03d}'] = eval_ARI(G, cluster_walktrap(embedding, method, False))\n",
    "        results[f'snapshotI_KL_{steps:03d}'] = eval_ARI(G, KL_symmetric(embedding, method, False))\n",
    "        results[f'snapshotI_wasserstein_{steps:03d}'] = eval_ARI(G, wasserstein(embedding, method, False))\n",
    "\n",
    "    # damped markov embedding\n",
    "    all_damps = [.1,.2,.3,.4,.5,.6,.7,.8,.85,.9,1]\n",
    "    for damp in all_damps:\n",
    "        embedding = embed_damped_markov(G, damp, steps=100)\n",
    "        dampstr = str(damp).replace('.','p')\n",
    "        results[f'damped_walktrap_{dampstr}'] = eval_ARI(G, cluster_walktrap(embedding, method, False))\n",
    "        results[f'damped_KL_{dampstr}'] = eval_ARI(G, KL_symmetric(embedding, method, False))\n",
    "        results[f'damped_wasserstein_{dampstr}'] = eval_ARI(G, wasserstein(embedding, method, False))\n",
    "\n",
    "    with open(f'results/ARI_{method}_{G.graph[\"name\"]}.pkl', 'wb') as f: \n",
    "        dump(results, f, HIGHEST_PROTOCOL)\n",
    "\n",
    "def do_girvannewman(G):\n",
    "    result = eval_ARI(G, girvan_newman(G))\n",
    "    with open(f'results/ARI_girvannewman_{G.graph[\"name\"]}.pkl', 'wb') as f: \n",
    "        dump(results, f, HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karate single\n",
      "karate complete\n"
     ]
    }
   ],
   "source": [
    "graphs = [wrangle_karate(), wrangle_football(), wrangle_caltech(), generate_lfr()]\n",
    "methods = ['single', 'complete', 'average', 'weighted', 'ward']\n",
    "\n",
    "for graph in graphs:\n",
    "    for method in methods:\n",
    "        print(f'{graph.graph[\"name\"]} {method}')\n",
    "        do_experiment(graph, method)\n",
    "\n",
    "for graph in graphs:\n",
    "    print(f'{graph.graph[\"name\"]} gn')\n",
    "    do_girvannewman(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results of above\n",
    "\n",
    "def plot_experiment(name, method, total=None):\n",
    "    with open(f'results/ARI_{method}_{name}.pkl', 'rb') as f:\n",
    "        results = load(f)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3)\n",
    "    fig.suptitle(name, fontweight='bold')\n",
    "    colmap = {\"snapshotI\":0, \"snapshotD\":1, \"damped\":2}\n",
    "    rowmap = {\"walktrap\":0, \"KL\":1, \"wasserstein\":2}\n",
    "    \n",
    "    bestARI = max((res[0] if res[0] is not None else -1) for res in results.values())\n",
    "    lines = {}\n",
    "    for label, result in sorted(results.items()):\n",
    "        \n",
    "        if result[0] is None:\n",
    "            continue # when KL does not converge\n",
    "        if result[0] == bestARI:\n",
    "            print(f'TODO HIGHLIGHT {label} {result[0]:.3f} {name}')\n",
    "            \n",
    "        split = label.split('_')\n",
    "        if len(split)==3:\n",
    "            ax_col = colmap[split[0]]\n",
    "            ax_row = rowmap[split[1]]\n",
    "            x = float(split[2].replace('p','.'))\n",
    "            y = result[0]\n",
    "            coord = (ax_row,ax_col)\n",
    "            if coord not in lines.keys():\n",
    "                lines[coord] = ([x], [y])\n",
    "            else:\n",
    "                lines[coord][0].append(x)\n",
    "                lines[coord][1].append(y)\n",
    "        else:\n",
    "            print(f\"{label:<25} {result[0]:.3f} {result[1]} {name}\")\n",
    "            # TODO: put this in another graph\n",
    "        \n",
    "        if total is not None:\n",
    "            if label not in total.keys():\n",
    "                total[label] = [result]\n",
    "            else:\n",
    "                total[label].append(result)\n",
    "\n",
    "    # plot lines and remove irrelevant ticks\n",
    "    for coord,line in lines.items():\n",
    "        axis = axes[coord[0],coord[1]]\n",
    "        axis.plot(line[0], line[1])\n",
    "        axis.set_ylim(-.1,1.1)\n",
    "        axis.grid(True)\n",
    "        \n",
    "        if coord[0] < 2:\n",
    "            axis.set_xticklabels(['' for _ in axis.get_xticklabels()])\n",
    "        if coord[1] > 0:\n",
    "            axis.set_yticklabels(['' for _ in axis.get_yticklabels()])\n",
    "        if coord[1] < 2:\n",
    "            axis.set_xlim(0,10)\n",
    "        else:\n",
    "            axis.set_xlim(0,1)\n",
    "        \n",
    "    # label relevant axis\n",
    "    for label,col in colmap.items():\n",
    "        axes[0,col].title.set_text(label)\n",
    "    for label,row in rowmap.items():\n",
    "        axes[row,0].set_ylabel(label)\n",
    "    \n",
    "    fig.savefig(f'figures/ARI_{method}_{name}.png')\n",
    "    \n",
    "def plot_experiments(names, method):\n",
    "    total_ARI = {}\n",
    "    for name in names:\n",
    "        plot_experiment(name, method, total_ARI)\n",
    "    \n",
    "    for label,results in total_ARI.items():\n",
    "        n = len(results)\n",
    "        if n==len(names):\n",
    "            total_ARI[label] = (sum(res[0] for res in results) / n, sum(res[1] for res in results) / n)\n",
    "        else:\n",
    "            total_ARI[label] = (-1,0)\n",
    "        with open(f'results/ARI_{method}_mean.pkl', 'wb') as f: \n",
    "            dump(total_ARI, f, HIGHEST_PROTOCOL)\n",
    "    plot_experiment('mean', method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [g.graph['name'] for g in graphs]\n",
    "# for method in methods:\n",
    "#     plot_experiment(names, method)\n",
    "plot_experiments(names, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustering(G, draw_dend=False):\n",
    "    embedding = embed_snapshot_markov(G, 5, degrees=False)\n",
    "    Z = cluster_walktrap(embedding, True)\n",
    "    print(eval_ARI(G, Z))\n",
    "\n",
    "    if draw_dend:\n",
    "        draw_dendrogram(G, Z)\n",
    "    root = to_tree(Z)\n",
    "    with open(f'dendrogram_{G.graph[\"name\"]}.pkl', 'wb') as f:\n",
    "        dump((G,root), f, HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_clustering(wrangle_karate(), 'karate')\n",
    "save_clustering(wrangle_football(), 'football')\n",
    "save_clustering(wrangle_caltech(), 'caltech')\n",
    "save_clustering(generate_lfr(), 'lfr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try infomap\n",
    "\n",
    "from infomap import Infomap\n",
    "im = Infomap()\n",
    "G = wrangle_caltech()\n",
    "for i,j in G.edges:\n",
    "    im.add_link(i,j)\n",
    "im.run(\"--two-level --num-trials 5\")\n",
    "predicted = []\n",
    "for node in im.tree:\n",
    "    if node.is_leaf:\n",
    "        predicted.append(node.module_id)\n",
    "#         print(node.node_id, node.module_id)\n",
    "\n",
    "gtruth = list(nx.get_node_attributes(G, 'cluster').values())\n",
    "print(adjusted_rand_score(gtruth, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try pure modularity\n",
    "\n",
    "from community import best_partition\n",
    "G = wrangle_caltech()\n",
    "predicted = list(best_partition(G).values())\n",
    "\n",
    "gtruth = list(nx.get_node_attributes(G, 'cluster').values())\n",
    "print(adjusted_rand_score(gtruth, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lesmis figure\n",
    "\n",
    "G = wrangle_lesmis()\n",
    "\n",
    "embedding = embed_snapshot_markov(G, 5, degrees=False)\n",
    "Z = cluster_walktrap(embedding, True)\n",
    "# Z = girvan_newman(G)\n",
    "\n",
    "# TODO: save names of characters to place around circle!\n",
    "draw_dendrogram(G, Z, (8,6))\n",
    "plt.savefig('lesmis_dendrogram.png')\n",
    "root = to_tree(Z)\n",
    "with open(f'dendrogram_lesmis.pkl', 'wb') as f:\n",
    "    dump((G,root), f, HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
