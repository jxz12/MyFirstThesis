\chapter{Nodes}
If one were to ask a random group of people to draw a network on a piece of paper, it is likely that most would draw dots to represent the nodes, and lines joining the dots to represent the edges. This is a representation so intuitive that it is often synonymous with the abstract concept of a network entirely.
This is the reason why this `join-the-dots' representation, known as the node-link diagram, is the most commonly studied and applied, and is also why it has been chosen for the purposes of this thesis.

However it is useful to have an idea of the other possible representations in order to gain a broader view of what network visualisation entails. A classical example of this includes the \emph{matrix plot}, which is a grid where each vertex is represented by a row and a column, and each edge is a dot filled in at the intersection of a row and column~\cite{TODO}.
A more obscure example is \emph{string graphs}, where each vertex is represented by a (possibly curved) line, and edges exist between vertices whose lines intersect.
Specialised types of networks can also have similarly specialised representations. Trees, for example, can be depicted as packed rectangles or circles~\cite{TODO}.
Graphs with low \emph{boxicity}, such as food webs~\cite{TODO}, can be drawn as \emph{intersection graphs} of overlapping lines or rectangles or cuboids.\footnote{Or hypercuboids, although the usefulness of that for visualisation is likely limited.}
A related and more common example is the \emph{disc graph}, where nodes are represented by circles and edges exist if the circles overlap. This sees widespread use as a Venn diagram, but usually without the connotation of network structure.
A gallery of such examples can be seen in Figure~\ref{TODO}; because of the difference between the abstract mathematical structure of a network and its representation within a visualisation, I will henceforth refer to the abstract structure itself as drawing graphs comprised of vertices and edges, and their representation as visualising networks comprised of nodes and links.\footnote{This is in line with the termininology chosen by the \emph{International Symposium on Graph Drawing and Network Visualisation} to separate its theoretical and applied submission tracks, and so I will attempt to adopt it here. However the distinction between the two can sometimes become blurred depending on the context.}

\begin{figure}
\caption{Each of these representations has its unique benefits and downsides. Matrix plots can show a very dense amount of data in a small space, but are very dependent on the ordering of rows and columns~\cite{TODO}.
The intersection-style graphs are intuitive, but cannot be drawn for all graphs~\cite{TODO}.}

\label{fig:graphrepresentations}
\end{figure}

Even within the subfield of node-link diagrams, there is a wide variety of options available.
Examples include \emph{arc} or \emph{chord diagrams}, where nodes are placed on a line or around a circle, respectively. Links are then added by drawing the eponymous arcs or chords between nodes.
A method that has recently gained popularity is the \emph{hive plot}, a simple but effective variant of parallel coordinate plots~\cite{TODO}, which places nodes on radial lines to draw curves between them, in a similar fashion to spider charts~\cite{TODO}. An important subtlety is that each node may or may not be placed on more than one line, and the order in which the nodes are spread across the line is also a conscious choice. This customisability is where the power of such a method lies.

Hopefully the above examples give a taste of how varied the representation of a network can be. In this chapter we will focus on 
"""
 From the 1980s, industrial demand for graph drawing algorithms has grown
– Software engineering: CASE systems, reverse engineering – Biology: PPI networks, gene regulatory networks
– Physical networks: network management tools
– Security: risk management, money movements
– Social network analysis
– Customer relationship management: value identification Many companies buy graph drawing algorithms, many code them.
Currently the international market for graph drawing algorithms is in the hundreds of millions of dollars per year.
"""

\section{Background}
The formal study of node-link diagrams dates back to least the 1920s. For example, F\'ary's theorem is a famous proof that any planar graph, defined as a graph that can be drawn without any intersecting links, can always be drawn in a planar way without needing links to be curved. This proof is attributed to F\'ary who published it in 1948~\cite{TODO}, and was independently discovered by Steinitz in 1922~\cite{TODO}, as well as a host of other authors in the same era~\cite{wagner1936, koebe1936, stein1951}.
The ensuing development of actual \emph{algorithms} for network layout emerged around the 1960s, with its seminal work widely attributed to the barycentre algorithm of Tutte in 1963~\cite{TODO}. 

Tutte's algorithm is very simple, and boils down to solving a system of linear equations, each of which strives to set a single node to the barycentre (i.e.\ mean position) of its neighbours. This is defined as 
\begin{equation}
    \mathbf{X}_i = \frac{1}{|N(i)|}\sum_{j\in N(i)}\mathbf{X}_j
\label{eq:tutte}
\end{equation}
where $\mathbf{X}_i$ is the $i$\textsuperscript{th} row of the $n\times k$ matrix $\mathbf{X}$, with $n$ being the number of vertices and $k$ the dimensionality of the layout (usually $2$), and $N(i)$ is the set of vertices neighbouring $i$.
Note that a necessary initial step is to fix the position of a selection of nodes around the boundary of the drawing, in order to avoid the trivial solution of placing all nodes in the same position.
The powerful insight that Tutte revealed with his algorithm was a remarkable mathematical theorem attached to it. He proved that this barycentre algorithm will always produce a planar drawing, in the specific case that the graph is planar and tri-connected (i.e.\ the graph cannot be disconnected by removing two vertices, no matter which two are removed).

This algorithm has served as the springboard for two main branches of node layout algorithms. Since planarity has been shown to be one of, if not the most important markers for readability in node-link diagrams~\cite{todo}, the first branch is a line of planarity-based algorithms, such as TODO~\cite{deFraysseix-Pach-Pollack} \cite{Chrobak} independently \cite{Schnyder}
A common problem with these methods is \emph{resolution}...
$\mathcal{NP}$-hard

The second branch comes from an intuitive interpretation of the algorithm: that each edge is analogous to a spring of natural length 0, where the solution to Tutte's system of linear equations can be seen as the point at which the elastic energy of these springs, according to Hooke's law~\cite{hooke}, is minimised. This energy is defined as
\begin{equation}
    \mathrm{energy}(\mathbf{X}) = \sum_{\{i,j\}\in E}||\mathbf{X}_j-\mathbf{X}_i||^2
\label{eq:tutte_energy}
\end{equation}
where $E$ is the set of all edges. Differentiating with respect to the position of a single node $\mathbf{X}_i$ results in
\begin{equation}
    \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X}) = \sum_{j\in N(i)}-2(\mathbf{X}_j-\mathbf{X}_i)
\label{eq:tutte_force}
\end{equation}
where it can be seen that setting the left-hand side to zero results in Equation~\eqref{eq:tutte}, corresponding to an embedding of minimum global energy in the system.

This interpretation has been taken and advanced to alleviate the resolution problem present in planarity based methods, by introducing the trade-off of foregoing mathematical rigour. 
This is done by using human intuition to formulate variations on Equation~\eqref{eq:tutte_energy}, in what are known as \emph{force-directed} algorithms.

\subsection{\texorpdfstring{\st{Force-directed}{ Optimisation algorithms}}{}}
I will now present an overview of the various techniques that have been sprouted from this second branch of algorithms, around which the work in this chapter is based. I will also frame them all within the context of \emph{optimisation}, a framework which will tie together otherwise loosely-connected threads in a logical taxonomy. 

The earliest of these algorithms was developed in 1984 by Eades~\cite{TODO} who, inspired by techniques for positioning transistors on integrated circuits~\cite{VLSI}, made two modifications. The first was to alter the edge springs in order to give them a non-zero natural length, avoiding having to fix an often arbitrary selection of nodes around the boundary. The second was to introduce a repulsive force between pairs of non-adjacent vertices to spread nodes evenly around the drawing.
The combination of these forces on a single node $i$ is defined as
\begin{equation}
    \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X}) = -\sum_{j\in N(i)}c_1\log(||\mathbf{X}_j-\mathbf{X}_i||)\overrightarrow{\mathbf{X}_{ij}}
    + \sum_{j\notin N(i)}\frac{c_2}{||\mathbf{X}_j-\mathbf{X}_i||^2}\overrightarrow{\mathbf{X}_{ij}}
\label{eq:eades}
\end{equation}
where $\overrightarrow{\mathbf{X}_{ij}} = \frac{\mathbf{X}_j-\mathbf{X}_i}{||\mathbf{X}_j-\mathbf{X}_i||}$ i.e.\ the normalised vector pointing from $i$ to $j$, and $c_1$ and $c_2$ are constant parameters determining the relative strengths of the forces. The first summation defines the `springs', where the logarithm attempts to maintain the spring at unit length by flipping to negative if the node pair gets too close together.\footnote{Hooke's law was abandoned here because ``\emph{Experience shows that Hookes Law (linear) springs are too strong when the vertices are far apart; the logarithmic force solves this problem}"~\cite{eades}.}
The second summation is the repulsive force, which always pushes $i$ away from $j$ if there does not exist an edge between them, and decays according to an inverse square function analogous to charged electrons obeying Coulomb's law~\cite{coulomb}.

There are two important aspects to notice here. The first is that the left hand side is not the energy itself, but its derivative. The second is that this derivative can no longer be straightforwardly solved as in Equation~\eqref{eq:tutte} because it has become \emph{non-linear}. How then is energy minimised? Through a method known as \emph{gradient descent}, which is as simple as iteratively moving nodes in the direction opposite to the derivative in Equation~\eqref{eq:eades} according to
\begin{equation}
    \mathbf{X}_i \leftarrow -\eta\, \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X})
\end{equation}
where $\eta$ is a constant parameter.
This operation, despite its simplicity, is theoretically proven to find a minimal energy embedding~\cite{gradientdescent}, although it is important to note that this embedding may not be globally optimal because the energy in~\eqref{eq:eades} is \emph{non-convex} and therefore may contain many local minima.

This optimisation through gradient descent interpretation is not how this type of algorithm is commonly presented, as  force-directed algorithms are often categorised into two families: force-balancing and energy-minimising models~\cite{ortmann, brandes_physical}.
Eades' model fits into the former, while others, such as the `stress' model that this chapter will focus heavily on, fit into the latter by directly defining an energy function such as the one in~\eqref{eq:tutte_energy}. Energy is then minimised by gradient descent or another optimisation technique, as will be further elaborated upon further into this chapter.
With the view that the objective of `force-balancing' is equivalent to gradient descent on another energy function, however, it is made clear that the two families are equivalent in the sense that both strive for the same goal: minimising energy.
% This unification also reveals exactly why heuristics, such as the `adaptive cooling scheme' of Hu~\cite{hu_efficient}, work.
%It is therefore possible to reverse-engineer the derivative into the energy function to objectively compare which optimisers do better

An optimisation-centric viewpoint also allows a variety of linear algebra methods based on finding eigenvalues to be grouped into the same taxonomy.
The high-dimensional embedding algorithm of Harel and Koren~\cite{hde} first embeds a graph in a high-dimensional space, determined by graph-theoretic or shortest-path distances, and then finds an optimal projection down to two dimensions by means of principal components analysis (PCA)~\cite{pca}.
The `classical scaling' approach of Brandes and Pich~\cite{brandes_eigensolver} is based on first transforming this high-dimensional embedding into a \emph{distance matrix} between high-dimensional points and then performing PCA.
The \emph{Spectral} approach, originally developed by Hall in 1970~\cite{hall} and largely ignored by the field until Koren in 2003~\cite{koren_spectral}, uses the graph Laplacian as the high-dimensional matrix instead of shortest-paths, upon which PCA is again directly applied. This approach is particularly closely related to Tutte's algorithm, which in fact also utilises the graph Laplacian, where the only difference is that the spectral approach requires that the variance of the embedding is non-zero, thereby avoiding the need to fix certain nodes around the boundary~\cite{koren_spectral}.
All of these methods boil down to finding the eigenvalues of a certain matrix, but each 

There have been many attempts at improving Eades' original formulation. For example a popular improvement is the algorithm of Fruchterman and Reingold, who 

"""
Force-directed methods account for 90\% of commercial and free graph drawing software for undirected graphs
"""
this is because they are intuitive and easily modified to include domain-specific features.


Introduce Kamada-Kawai 1989 and Fruchterman-Reingold 1991 GEM Frick 1995
GEM is fruchterman-reingold but with a zillion extra convergence heuristics
A popular improvement to this came in 


\subsection{Stress in detail}
\begin{equation}
    \mathrm{stress}(\mathbf{X}) = \sum_{\{i,j\}:i<j}w_{ij}(||\mathbf{X}_i-\mathbf{X}_j||-d_{ij})^2
\end{equation}


\section{Stochastic Gradient Descent}
SGD
recommended cooling schedule changes for different things
\subsection{Results}
comparison to majorization
\subsection{Parameterisation}
finding $\eta_{\max}$ and $\varepsilon$.
\subsection{Large Graphs}
various bottlenecks
khoury low rank stress majorization is different from subspace optimisation in graphviz
normal MDS community has some options like Halko et al. or GLINT, but an additional problem with graphs is that shortest paths still need to be computed, and so we cannot presume that the entire distance matrix is available to us.
multisource shortest paths is still used to find regions
but we present a simpler algorithm to find weights (make pseudocode)
note that it does not really minimise a particular function because the weights are asymmetric.

\section{Cookbook}
tweaks to the algorithm for various applications
\subsection{Radial Layout}
cool much more than usual for convergence
\subsection{Fixing one dimension}
adding extra initial iterations can help
setting $\mu_{\max}=1.1$ is good too
\subsection{Regular Multidimensional Scaling}
show the digits dataset (note that weights are all set to 1)
try Phate classical MDS stuff
graphviz optimises in a subspace first?