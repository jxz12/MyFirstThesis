\chapter{Nodes}
If one were to ask a random group of people to draw a network on a piece of paper, it is likely that most would draw dots to represent the nodes, and lines joining the dots to represent the edges. This is a representation so intuitive that it is often synonymous with the abstract concept of a network entirely.
This is the reason why this `join-the-dots' representation, known as the node-link diagram, is the most commonly studied and applied, and is also why it has been chosen for the purposes of this thesis.

However it is useful to have an idea of the other possible representations in order to gain a broader view of what network visualisation entails. A classical example of this includes the \emph{matrix plot}, which is a grid where each vertex is represented by a row and a column, and each edge is a dot filled in at the intersection of a row and column~\cite{TODO}.
A more obscure example is \emph{string graphs}, where each vertex is represented by a (possibly curved) line, and edges exist between vertices whose lines intersect.
Specialised types of networks can also have similarly specialised representations. Trees, for example, can be depicted as packed rectangles or circles~\cite{TODO}.
Graphs with low \emph{boxicity}, such as food webs~\cite{TODO}, can be drawn as \emph{intersection graphs} of overlapping lines or rectangles or cuboids.\footnote{Or hypercuboids, although the usefulness of that for visualisation is likely limited.}
A related and more common example is the \emph{disc graph}, where nodes are represented by circles and edges exist if the circles overlap. This sees widespread use as a Venn diagram, but usually without the connotation of network structure.
A gallery of such examples can be seen in Figure~\ref{TODO}; because of the difference between the abstract mathematical structure of a network and its representation within a visualisation, I will henceforth refer to the abstract structure itself as drawing graphs comprised of vertices and edges, and their representation as visualising networks comprised of nodes and links.\footnote{This is in line with the termininology chosen by the \emph{International Symposium on Graph Drawing and Network Visualisation} to separate its theoretical and applied submission tracks, and so I will attempt to adopt it here. However the distinction between the two can sometimes become blurred depending on the context.}

\begin{figure}
\caption{Each of these representations has its unique benefits and downsides. Matrix plots can show a very dense amount of data in a small space, but are very dependent on the ordering of rows and columns~\cite{TODO}.
The intersection-style graphs are intuitive, but cannot be drawn for all graphs~\cite{TODO}.}

\label{fig:graph_representations}
\end{figure}

Even within the subfield of node-link diagrams, there is a wide variety of options available.
Examples include \emph{arc} or \emph{chord diagrams}, where nodes are placed on a line or around a circle, respectively. Links are then added by drawing the eponymous arcs or chords between nodes.
A method that has recently gained popularity is the \emph{hive plot}, a simple but effective variant of parallel coordinate plots~\cite{TODO}, which places nodes on radial lines to draw curves between them, in a similar fashion to spider charts~\cite{TODO}. An important subtlety is that each node may or may not be placed on more than one line, and the order in which the nodes are spread across the line is also a conscious choice. This customisability is where the power of such a method lies.

Hopefully the above examples give a taste of how varied the representation of a network can be. In this chapter we will focus on 
"""
 From the 1980s, industrial demand for graph drawing algorithms has grown
– Software engineering: CASE systems, reverse engineering – Biology: PPI networks, gene regulatory networks
– Physical networks: network management tools
– Security: risk management, money movements
– Social network analysis
– Customer relationship management: value identification Many companies buy graph drawing algorithms, many code them.
Currently the international market for graph drawing algorithms is in the hundreds of millions of dollars per year.
"""

\section{Background}
\label{sec:nodes_background}
The formal study of node-link diagrams dates back to least the 1920s. For example, F\'ary's theorem is a famous proof that any planar graph, defined as a graph that can be drawn without any intersecting links, can always be drawn in a planar way without needing links to be curved. This proof is attributed to F\'ary who published it in 1948~\cite{TODO}, and was independently discovered by Steinitz in 1922~\cite{TODO}, as well as a host of other authors in the same era~\cite{wagner1936, koebe1936, stein1951}.
The ensuing development of actual \emph{algorithms} for network layout emerged around the 1960s, with its seminal work widely attributed to the barycentre algorithm of Tutte in 1963~\cite{TODO}. 

Tutte's algorithm is very simple, and boils down to solving a system of linear equations, each of which strives to set a single node to the barycentre (i.e.\ mean position) of its neighbours. This is defined as 
\begin{equation}
    \mathbf{X}_i = \frac{1}{|N(i)|}\sum_{j\in N(i)}\mathbf{X}_j
\label{eq:tutte}
\end{equation}
where $\mathbf{X}_i$ is the $i$\textsuperscript{th} row of the $n\times k$ matrix $\mathbf{X}$, with $n$ being the number of vertices and $k$ the dimensionality of the layout (usually $2$), and $N(i)$ is the set of vertices neighbouring $i$.
Note that a necessary initial step is to fix the position of a selection of nodes around the boundary of the drawing, in order to avoid the trivial solution of placing all nodes in the same position.
The powerful insight that Tutte revealed with his algorithm was a remarkable mathematical theorem attached to it. He proved that this barycentre algorithm will always produce a planar drawing, in the specific case that the graph is planar and tri-connected (i.e.\ the graph cannot be disconnected by removing two vertices, no matter which two are removed).

This algorithm has served as the springboard for two main branches of network layout algorithms. Since planarity has been shown to be one of, if not the most important markers for readability in node-link diagrams~\cite{purchase}, the first branch is a line of planarity-based algorithms which can guarantee planar node layouts. Examples include the algorithm of Read~\cite{read1987} which recursively removes nodes and adds dummy edges to maintain a triangulated graph at each step.  A problem with this method and Tutte's is \emph{node resolution}, which means that nodes may be placed exponentially close to each other, rendering the graph impossible to read in certain areas.
A breakthrough for this problem came in 1990 by de Fraysseix et al.~\cite{deFraysseix-Pach-Pollack1990} who described the first layout algorithm achieve an asymptotically optimal node resolution of being able to layout nodes on an $\mathcal{O}(n\times n)$ grid~\cite{Chrobak}. This was done by constructing a \emph{canonical ordering} of the graph~\cite{todo} which is used to order nodes on the x-axis, and from there y-axis positions can be chosen such to avoid crossings.
Improvements and refinements have been made with this algorithm in mind~\cite{Chrobak1995, Zhang2005}, but none have managed to solve the problem of poor \emph{angular resolution}, which means that adjacent edges form small angles between each other, another property that has been shown to impact the readability of layouts~\cite{Huang2008}.
This issue exists until this day~\cite{eadeshowtodraw}, and is a primary reason why most network visualisation software uses algorithms based on a second, less mathematically rigorous branch of algorithms inspired by Tutte.

This second branch comes from an intuitive interpretation of Tutte's algorithm: that each edge is analogous to a spring of natural length 0, where the solution to Tutte's system of linear equations can be seen as the point at which the elastic energy of these springs, according to Hooke's law~\cite{hooke}, is minimised. This energy is defined as
\begin{equation}
    \mathrm{energy}(\mathbf{X}) = \sum_{\{i,"j\}\in E}||\mathbf{X}_j-\mathbf{X}_i||^2
\label{eq:tutte_energy}
\end{equation}
where $E$ is the set of all edges. Differentiating with respect to the position of a single node $\mathbf{X}_i$ results in
\begin{equation}
    \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X}) = \sum_{j\in N(i)}-2(\mathbf{X}_j-\mathbf{X}_i)
\label{eq:tutte_force}
\end{equation}
where it can be seen that setting the left-hand side to zero results in Equation~\eqref{eq:tutte}, corresponding to an embedding of minimum global energy in the system.

This interpretation has been taken and advanced to alleviate the resolution problem present in planarity based methods, by introducing the trade-off of foregoing mathematical rigour. 
This is done by using human intuition to formulate variations on Equation~\eqref{eq:tutte_energy}, in what are known as \emph{force-directed} algorithms.

\subsection{\texorpdfstring{\st{Force-directed}{ Optimisation algorithms}}{}}
\label{sec:force_background}
I will now present an overview of the various methods that have been sprouted from this second branch of algorithms, around which the work in this chapter is based. I will also frame them all within the context of \emph{optimisation}, a framework which will tie together otherwise loosely-connected threads in a logical taxonomy.
It will also lead more cogently into the novel contributions described in Section~\ref{sec:sgd}.

\begin{figure}
    \caption{A binary tree of depth TODO visualised using some of the algorithms described in Section~\ref{sec:force_background}. The corresponding equations from left to right are: Tutte~\eqref{eq:tutte}, Eades~\eqref{eq:eades}, spectral~\eqref{eq:spectral}, and stress~\eqref{eq:stress}}.
    Note that...
    \label{fig:force_layouts}
\end{figure}

The earliest of these algorithms was developed in 1984 by Eades~\cite{TODO} who, inspired by techniques for positioning transistors on integrated circuits~\cite{VLSI}, made two modifications. The first was to alter the edge springs in order to give them a non-zero natural length, avoiding having to fix an often arbitrary selection of nodes around the boundary. The second was to introduce a repulsive force between pairs of non-adjacent vertices to spread nodes evenly around the drawing.
The combination of these forces on a single node $i$ is defined as
\begin{equation}
    \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X}) = -\sum_{j\in N(i)}c_1\log(||\mathbf{X}_j-\mathbf{X}_i||)\overrightarrow{\mathbf{X}_{ij}}
    + \sum_{j\notin N(i)}\frac{c_2}{||\mathbf{X}_j-\mathbf{X}_i||^2}\overrightarrow{\mathbf{X}_{ij}}
\label{eq:eades}
\end{equation}
where $\overrightarrow{\mathbf{X}_{ij}} = \frac{\mathbf{X}_j-\mathbf{X}_i}{||\mathbf{X}_j-\mathbf{X}_i||}$ i.e.\ the normalised vector pointing from $i$ to $j$, and $c_1$ and $c_2$ are constant parameters determining the relative strengths of the forces. The first summation defines the `springs', where the logarithm attempts to maintain the spring at unit length by flipping to negative if the node pair gets too close together.\footnote{Hooke's law was abandoned here because ``\emph{Experience shows that Hookes Law (linear) springs are too strong when the vertices are far apart; the logarithmic force solves this problem}"~\cite{eades}.}
The second summation is the repulsive force, which always pushes $i$ away from $j$ if there does not exist an edge between them, and decays according to an inverse square function analogous to charged electrons obeying Coulomb's law~\cite{coulomb}.

There are two important aspects to notice here. The first is that the left-hand side is not the energy itself, but its derivative. The second is that this derivative can no longer be straightforwardly solved as in Equation~\eqref{eq:tutte} because it has become \emph{non-linear}. How then is energy minimised? Through a method known as \emph{gradient descent}, which is as simple as iteratively moving nodes in the direction opposite to the derivative in Equation~\eqref{eq:eades} according to
\begin{equation}
    \mathbf{X}_i \leftarrow -\eta\, \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X})
\label{eq:gradient_descent}
\end{equation}
where $\eta$ is a constant parameter.
This operation, despite its simplicity, is theoretically proven to find a minimal energy embedding~\cite{gradientdescent}, although it is important to note that this embedding may not be globally optimal because the energy function defined by~\eqref{eq:eades} is \emph{non-convex} and therefore may contain many \emph{local minima}. Many of the concepts introduced in this paragraph will be elaborated upon in Section~\ref{sec:stress_background}.

This optimisation through gradient descent interpretation is not how this type of algorithm is commonly presented, as  force-directed algorithms are often categorised into two families: force-balancing and energy-minimising models~\cite{ortmann, brandes_physical}.
Eades' model fits into the former, while others fit into the latter by directly defining an energy function such as in~\eqref{eq:tutte_energy}.
Energy is then minimised by gradient descent as in Equation~\eqref{eq:gradient_descent} or by another optimisation technique, as will be further elaborated upon in Section~\ref{sec:stress_background}.
With the view that the action of `force-balancing' is equivalent to gradient descent on another energy function, however, it is made clear that the two families are equivalent in the sense that both strive for the same goal: minimising energy.
% This unification also reveals exactly why heuristics, such as the `adaptive cooling scheme' of Hu~\cite{hu_efficient}, work.
%It is therefore possible to reverse-engineer the derivative into the energy function to objectively compare which optimisers do better

This optimisation-centric viewpoint also allows a variety of linear algebra-based methods to be grouped into the same taxonomy.
The high-dimensional embedding algorithm of Harel and Koren~\cite{hde} first embeds a graph in a high-dimensional space, determined by a covariance matrix of graph-theoretic or shortest-path distances, and then finds an optimal projection down to two dimensions by means of principal components analysis (PCA)~\cite{pca}.
The `classical scaling' approach of Brandes and Pich~\cite{brandes_eigensolver} is based on constructing the high-dimensional embedding from a double centered \emph{distance matrix} between high-dimensional points and then performing PCA.
The \emph{Spectral} approach, originally developed by Hall in 1970~\cite{hall} and largely ignored by the field until Koren in 2003~\cite{koren_spectral}, uses a graph Laplacian as the high-dimensional matrix instead of shortest-paths, upon which PCA is again applied. This approach is particularly closely related to Tutte's algorithm, which in fact also utilises a graph Laplacian for Equation~\eqref{eq:tutte}, where the difference is that the spectral approach requires the variance of the embedding to be non-zero, thereby avoiding the need to fix certain nodes around the boundary~\cite{koren_spectral}.
Specifically its energy upon a single dimension of the layout in the column vector $\mathbf{x}$ can be intuitively derived as
\begin{align}
\begin{split}
    \mathrm{energy}(\mathbf{x}) &= \sum_{\{i,j\}\in E}w_{ij}(\mathbf{x}_j-\mathbf{x}_i)^2\\
    \mathrm{var}(\mathbf{x}) &= \frac{1}{n}\sum_i(\mathbf{x}_i-\overline{\mathbf{x}})^2 = 1
\end{split}
\label{eq:spectral}
\end{align}
where $w_{ij}$ is an optional weight given to each edge (previously all set to 1 in~\eqref{eq:tutte}) and $\overline{\mathbf{x}}$ is the mean of $\mathbf{x}$. The one-dimensional layout that minimises energy is equal to the second smallest eigenvector of the Laplacian matrix $\mathbf{L}$, defined as
\begin{equation}
    \mathbf{x}^T\mathbf{Lx} = \mathrm{energy}(\mathbf{x})
\end{equation}
where $\mathbf{x}^T$ denotes the transpose of $\mathbf{x}$. Subsequent dimensions are found by calculating subsequent eigenvectors of $\mathbf{L}$, often through power iteration~\cite{koren_spectral}.
All of these methods boil down to finding the eigendecomposition of a certain matrix which describes the graph in a different way, and each set of eigenvectors describes an optimal projection into a lower dimension.

The linear nature of such methods is a double-edged sword, as it allows them to be calculated and/or approximated quickly and precisely, but they all suffer from the same issue that resulting layouts under-represent local detail~\cite{brandes_study}, especially in examples such as the binary tree in Figure~\ref{fig:force_layouts}. However their effectiveness at capturing global structure consistently has led to them being commonly used as an initialisation step~\cite{brandes_study}.

Circling back to non-linear methods, there have been many attempts at improving Eades' original formulation. A widely-used alternative, developed in 1991, is the method of Fruchterman and Reingold~\cite{fruchterman} who revert springs back to having a natural length of 0, but balance this out by applying the repulsive force between all pairs of vertices. Counterintuitively, increasing the number of repulsive terms also makes the summation more efficient to calculate, by taking inspiration from techniques used in physical simulations. More detail on scaling layout algorithms to large graphs can be found in Section~\ref{sec:large_graphs}.
Another notable attempt came from Frick et al.~\cite{frick} in 1995, who altered the calculations to never require a square root operation, and introduced a number of extra heuristics to speed up the convergence of the iterative optimisation. These heuristics were introduced on an intuitive and ad hoc basis, but have been shown to greatly speed up convergence in practice~\cite{brandes_review}.
% \begin{equation}
%     \frac{d}{d\mathbf{X}_i}\mathrm{energy}(\mathbf{X}) = -\sum_{j\in N(i)}\frac{c^2}{||\mathbf{X}_j-\mathbf{X}_i||}\overrightarrow{\mathbf{X}_{ij}}
%     + \sum_{j:j\neq i}\frac{||\mathbf{X}_j-\mathbf{X}_i||^2}{c}\overrightarrow{\mathbf{X}_{ij}}
% \end{equation}

The energy function that will become the focus for the rest of this chapter is known as \emph{stress}, and was first used within the context of graph layout in 1989 by Kamada and Kawai~\cite{kamada}. Its energy is defined as
\begin{equation}
    \mathrm{stress}(\mathbf{X}) = \sum_{\{i,j\}\:i<j}w_{ij}(||\mathbf{X}_j-\mathbf{X}_i||-d_{ij})^2
\label{eq:stress}
\end{equation}
where $w_{ij}$ and $d_{ij}$ are constants specific to each term in the summation. The intuition behind this formulation is that there are Hooke's law springs attached between all pairs of vertices, not just those connected by edges. Each of these springs is of natural length $d_{ij}$ and of stiffness $w_{ij}$. In practice, $d_{ij}$ is usually set to the shortest-path distance between vertices, and $w_{ij}$ is almost always set to $d_{ij}^{-2}$ in order to suppress the contribution from long-range springs that would otherwise obfuscate local detail~\cite{brandes_review}.

Stress as an energy function is known to produce high-quality layouts~\cite{brandes_review, todo} without requiring extra input parameters such as those in Equation~\eqref{eq:eades}. However, it is known to be difficult to optimise due to an abundance of local minima~\cite{onedimensionalmds, hu}, and also does not scale well to larger graphs due to the quadratic number of terms in the summation~\cite{brandes_review, hu}.
The subsequent work throughout this chapter will aim at addressing these issues through the application of an algorithm, widely used in other fields but not previously applied to Equation~\ref{eq:stress}, known as \emph{stochastic gradient descent}.
Before describing sgd I will... first detail the history of optimisation techniques applied to~\eqref{eq:stress}
TODO

% """
% Force-directed methods account for 90\% of commercial and free graph drawing software for undirected graphs~\cite{eades_revisited}
% """
% this is because they are intuitive and easily modified to include domain-specific features~\cite{magnets and shit}.

\subsection{Multidimensional Scaling}
\label{sec:stress_background}
Before 

\subsection{Constraint Relaxation}
\label{sec:wcr}
lol at the conception
dwyer

\section{Stochastic Gradient Descent}
\label{sec:sgd}
This is where the novel stuff starts SGD

\subsection{Experimental Results}
comparison to majorization

\subsection{Parameterisation}
finding $\eta_{\max}$ and $\varepsilon$.

\subsection{Randomisation}
random bits

\section{Large Graphs}
\label{sec:large_graphs}
various bottlenecks
khoury low rank stress majorization is different from subspace optimisation in graphviz
normal MDS community has some options like Halko et al. or GLINT, but an additional problem with graphs is that shortest paths still need to be computed, and so we cannot presume that the entire distance matrix is available to us.
multisource shortest paths is still used to find regions
but we present a simpler algorithm to find weights (make pseudocode)
note that it does not really minimise a particular function because the weights are asymmetric.

\subsection{Sparse stress}

\section{Cookbook}
tweaks to the algorithm for various applications
Nails can be used to hold a node in place~\cite{something}
Magnetic fields or magnetised springs can be used to align nodes in various ways
Attractive forces can be used to keep clusters together
\subsection{Radial Layout}
cool much more than usual for convergence
\subsection{Fixing one dimension}
adding extra initial iterations can help
setting $\mu_{\max}=1.1$ is good too, if not necessary
\subsection{Regular Multidimensional Scaling}
show the digits dataset (note that weights are all set to 1)
try Phate classical MDS stuff
graphviz optimises in a subspace first?